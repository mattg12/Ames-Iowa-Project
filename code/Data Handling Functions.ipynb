{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matthew Garton\n",
    "# Regression Project - Data Handling Functions\n",
    "\n",
    "### Purpose: Define functions to automate basic workflows for handling data, fitting, and running my models\n",
    "\n",
    "### Note: \n",
    "I have used some, but not all, of these functions in my project by copy and pasting them into the relevant notebook. I am sure this is not the most efficient way to do this. My goals to improve upon my work would be to finalize these functions and use them effectively, so that I can run through my data processing, model training, and model implementation as seamlessly as possible. This notebook is a starting point for that framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules - will clean this up later; starting with 'kitchen sink approach'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to handle basic data inspection\n",
    "def inspect_data(df):\n",
    "    '''Perfrom basic data inspection tasks, provide user with readable output'''\n",
    "    print('Shape: {}\\n'.format(df.shape))\n",
    "    print(df.info(),'\\n') \n",
    "    print('Null Counts: \\n',df.isnull().sum(),'\\n')\n",
    "    \n",
    "    return df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean the data\n",
    "def clean_ames_data(df):\n",
    "    '''Generalized function to clean a sample of Ames Housing Data'''\n",
    "    \n",
    "    # convert column names to useable format\n",
    "    df.columns = [x.lower().replace(' ','_') for x in df.columns]\n",
    "    \n",
    "    # drop 'id' and 'pid' columns\n",
    "    #df.drop(['id','pid'], axis=1, inplace=True)\n",
    "\n",
    "    # Dealing with NaN values. Handling the special case of Masonry Veneer Type first\n",
    "    \n",
    "    df['mas_vnr_type'].fillna(value = 'None', inplace = True) # Assuming 'NaN' should be 'None' for Masonry Type\n",
    "    df['mas_vnr_area'].fillna(value = 0.0, inplace = True) # Assuming masonry area is 0.0 for houses with 'NaN' type\n",
    "    \n",
    "    # for categorical variables, the missing values should actually be marked 'NA'\n",
    "    nulls = df.columns[df.isnull().any()]\n",
    "    for col in df[nulls].select_dtypes(include = 'object').columns:\n",
    "        df[col].fillna(value = 'NA', inplace = True)\n",
    "    \n",
    "    # filtering for houses with no basement, replacing numerical columns 'NaNs' with 0.0\n",
    "    no_bsmt = df['bsmt_qual'] == 'NA'    \n",
    "    for col in df[no_bsmt].filter(regex = 'bsmt'):\n",
    "        df[col].fillna(value = 0.0, inplace = True)\n",
    "        \n",
    "    # use the same procedure to handle numerical columns for houses with no garage\n",
    "    no_garage = df['garage_type'] == 'NA' \n",
    "    for col in df[no_garage].filter(regex = 'garage'):\n",
    "        df[col].fillna(value = 0.0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create the dummy variables I need\n",
    "def create_ames_dummies(df, columns):\n",
    "    '''Turn categorical variables into dummies'''\n",
    "\n",
    "    # specialized dummy to account for a specific house style\n",
    "    style_mask = df['house_style'] == '2.5Fin'\n",
    "    df['StyleDummy'] = np.where(df['house_style'] == '2.5Fin', 1, 0)\n",
    "\n",
    "    # specialized dummy for being adjacent to or near a positive feature\n",
    "    df['PosFeature'] = np.where((df['condition_2'] == 'PosN') | (df['condition_2'] == 'PosA'), 1, 0)\n",
    "\n",
    "    # Create categorical variable for Location\n",
    "    df['Location'] = df['neighborhood']\n",
    "    df['Location'].replace({'MeadowV':'Low','IDOTRR': 'Low','BrDale': 'Low','OldTown': 'Low',\n",
    "                                'Edwards':'Low','BrkSide':'Low', 'Landmrk': 'LowMed','Sawyer': 'LowMed',\n",
    "                                'SWISU':'LowMed','NAmes':'LowMed','NPkVill':'LowMed','Blueste':'LowMed',\n",
    "                                'Mitchel':'LowMed','Gilbert':'MedHigh','Greens':'MedHigh','SawyerW':'MedHigh',\n",
    "                                'NWAmes':'MedHigh','Blmngtn':'MedHigh','CollgCr':'MedHigh','ClearCr':'MedHigh',\n",
    "                                'Crawfor':'MedHigh','Somerst':'High','Timber':'High','Veenker':'High','GrnHill':'High',\n",
    "                                'NoRidge':'High','NridgHt':'High','StoneBr':'High'}, inplace = True)\n",
    "    \n",
    "    # one-hot encoding for other categorical variables\n",
    "    df = pd.get_dummies(df, columns = columns, drop_first = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to prepare data for modeling\n",
    "def prepare_training_data(df, predictors, target, scale_data = False, polynomial = False):\n",
    "    '''\n",
    "    Prepares training data for model fitting\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    df: DataFrame\n",
    "    predictors: list of columns to use as predictor variables\n",
    "    target: column to use as response variable\n",
    "    scale_data: default False; set to True to standard-scale data\n",
    "    polynomial: default False; set to True to generate polynomial features\n",
    "    '''\n",
    "    \n",
    "    clean_ames_data(df) # clean the data\n",
    "    create_ames_dummies(df) # create dummies\n",
    "    \n",
    "    # split data into X and y\n",
    "    X = df[predictors]\n",
    "    y = df[target]\n",
    "    \n",
    "    # use polynomial features, if appropriate\n",
    "    if polynomial == True:\n",
    "        poly = PolynomialFeatures(include_bias = False)\n",
    "        X = poly.fit_transform(X)\n",
    "        \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    # standard-scale data, if appropriate\n",
    "    if scale_data == True:\n",
    "        ss = StandardScaler()\n",
    "        X_train = ss.fit_transform(X_train)\n",
    "        X_test = ss.fit_transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to prepare testing data for the model\n",
    "def prepare_testing_data(df, predictors, train_df, scale_data = False, polynomial = False):\n",
    "    '''\n",
    "    Prepares testing data for use in prediction model. Ensure testing data has the same \n",
    "    format as the training data on which the model was fit.\n",
    "    \n",
    "    Params\n",
    "    -------\n",
    "    df: test DataFrame\n",
    "    predictors: list of columns to use as predictors\n",
    "    train_df: training data, used to compare testing data to\n",
    "    scale_data: default False; set to True to standard-scale data\n",
    "    polynomial: default False; set to True to generate polynomial features\n",
    "    '''\n",
    "    \n",
    "    clean_ames_data(df) # clean the data\n",
    "    create_ames_dummies(df) # create dummies\n",
    "    \n",
    "    # align testing data with training data to ensure no missing columns\n",
    "    df, train_df = train_df.align(df, join = 'outer', axis = 1, fill_value = 0.0)\n",
    "    \n",
    "    # create matrix of predictor variables\n",
    "    X = df[predictors]\n",
    "    \n",
    "    # use polynomial features, if appropriate\n",
    "    if polynomial == True:\n",
    "        poly = PolynomialFeatures(include_bias = False)\n",
    "        X = poly.fit_transform(X)\n",
    "    \n",
    "    # standard-scale data, if appropriate\n",
    "    if scale_data == True:\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a generalized model to deploy a model on new data\n",
    "def deploy_model(data, X, model, submission):\n",
    "    '''\n",
    "    Given prepared data, predictor matrix, and a model, use the model to predict response variable.\n",
    "    Export the result to a csv file in the appropriate directory'''\n",
    "    \n",
    "    data['salesprice_hat'] = model.predict(X) # predicy y\n",
    "    \n",
    "    # Re-format Id and SalePrice columns to meet Kaggle requirements\n",
    "    data.rename({'id': 'Id', 'salesprice_hat': 'SalePrice'}, axis = 1, inplace = True)\n",
    "    \n",
    "    result = data[['Id','SalePrice']] # store the relevant, formatted columns into a df\n",
    "    \n",
    "    # export the result to a csv in the appropriate directory\n",
    "    result.to_csv('../data/submission_{}.csv'.format(submission), index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to fit and evaluate the chosen model\n",
    "def fit_eval_model(X_train, X_test, y_train, y_test, kind = 'linear'):\n",
    "    '''\n",
    "    Given training and testing data, as well as a model selection, fit the model\n",
    "    using the training data, and evaluate the model on both training and testing data.\n",
    "    Return a model and a model evaluation\n",
    "    '''\n",
    "    \n",
    "    # Instantiate model\n",
    "    if kind == 'linear':\n",
    "        model = LinearRegression()\n",
    "    elif kind == 'lasso':\n",
    "        model = LassoCV()\n",
    "    elif kind == 'ridge':\n",
    "        model = RidgeCV()\n",
    "    elif kind == 'elastic net':\n",
    "        model = ElasticNetCV()\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    model = model.fit(X_train, y_train)\n",
    "    \n",
    "    # score on training data\n",
    "    print('Training Score: {}'.format(lr.score(X_train, y_train)))\n",
    "    \n",
    "    # score on test data\n",
    "    print('Testing Score: {}'.format(lr.score(X_test, y_test)))\n",
    "          \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
