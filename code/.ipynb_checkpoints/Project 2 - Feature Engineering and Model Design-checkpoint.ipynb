{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matthew Garton - DSI Boston\n",
    "# Regression Project - Feature Engineering\n",
    "\n",
    "### Purpose: notebook for selecting which variables to include in model as well as testing various model designs. \n",
    "\n",
    "Initially I had planned to maintain a separate notebook for feature engineering and model design (and may return to that framework in the future), but in this project, I found that feature selection and model design tended to blend together into one step, so I found it easier to do both simultaneously in one notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "ames = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean the data\n",
    "def clean_ames_data(df):\n",
    "    '''Generalized function to clean a sample of Ames Housing Data'''\n",
    "    \n",
    "    # convert column names to useable format\n",
    "    df.columns = [x.lower().replace(' ','_') for x in df.columns]\n",
    "    \n",
    "    # drop 'id' and 'pid' columns\n",
    "    #df.drop(['id','pid'], axis=1, inplace=True)\n",
    "\n",
    "    # Dealing with NaN values. Handling the special case of Masonry Veneer Type first\n",
    "    \n",
    "    df['mas_vnr_type'].fillna(value = 'None', inplace = True) # Assuming 'NaN' should be 'None' for Masonry Type\n",
    "    df['mas_vnr_area'].fillna(value = 0.0, inplace = True) # Assuming masonry area is 0.0 for houses with 'NaN' type\n",
    "    \n",
    "    # for categorical variables, the missing values should actually be marked 'NA'\n",
    "    nulls = df.columns[df.isnull().any()]\n",
    "    for col in df[nulls].select_dtypes(include = 'object').columns:\n",
    "        df[col].fillna(value = 'NA', inplace = True)\n",
    "    \n",
    "    # filtering for houses with no basement, replacing numerical columns 'NaNs' with 0.0\n",
    "    no_bsmt = df['bsmt_qual'] == 'NA'    \n",
    "    for col in df[no_bsmt].filter(regex = 'bsmt'):\n",
    "        df[col].fillna(value = 0.0, inplace = True)\n",
    "        \n",
    "    # use the same procedure to handle numerical columns for houses with no garage\n",
    "    no_garage = df['garage_type'] == 'NA' \n",
    "    for col in df[no_garage].filter(regex = 'garage'):\n",
    "        df[col].fillna(value = 0.0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "clean_ames_data(ames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into X an y\n",
    "X = ames.loc[:,ames.columns != 'saleprice']\n",
    "y = ames['saleprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical variable for Location\n",
    "X['Location'] = X['neighborhood']\n",
    "X['Location'].replace({'MeadowV':'Low','IDOTRR': 'Low','BrDale': 'Low','OldTown': 'Low',\n",
    "                                'Edwards':'Low','BrkSide':'Low', 'Landmrk': 'LowMed','Sawyer': 'LowMed',\n",
    "                                'SWISU':'LowMed','NAmes':'LowMed','NPkVill':'LowMed','Blueste':'LowMed',\n",
    "                                'Mitchel':'LowMed','Gilbert':'MedHigh','Greens':'MedHigh','SawyerW':'MedHigh',\n",
    "                                'NWAmes':'MedHigh','Blmngtn':'MedHigh','CollgCr':'MedHigh','ClearCr':'MedHigh',\n",
    "                                'Crawfor':'MedHigh','Somerst':'High','Timber':'High','Veenker':'High','GrnHill':'High',\n",
    "                                'NoRidge':'High','NridgHt':'High','StoneBr':'High'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to create the dummy variables I need\n",
    "def create_ames_dummies(df, columns):\n",
    "    '''Turn categorical variables into dummies'''\n",
    "\n",
    "    # specialized dummy to account for a specific house style\n",
    "    style_mask = df['house_style'] == '2.5Fin'\n",
    "    df['StyleDummy'] = np.where(df['house_style'] == '2.5Fin', 1, 0)\n",
    "\n",
    "    # specialized dummy for being adjacent to or near a positive feature\n",
    "    df['PosFeature'] = np.where((df['condition_2'] == 'PosN') | (df['condition_2'] == 'PosA'), 1, 0)\n",
    "\n",
    "    # Create categorical variable for Location\n",
    "    df['Location'] = df['neighborhood']\n",
    "    df['Location'].replace({'MeadowV':'Low','IDOTRR': 'Low','BrDale': 'Low','OldTown': 'Low',\n",
    "                                'Edwards':'Low','BrkSide':'Low', 'Landmrk': 'LowMed','Sawyer': 'LowMed',\n",
    "                                'SWISU':'LowMed','NAmes':'LowMed','NPkVill':'LowMed','Blueste':'LowMed',\n",
    "                                'Mitchel':'LowMed','Gilbert':'MedHigh','Greens':'MedHigh','SawyerW':'MedHigh',\n",
    "                                'NWAmes':'MedHigh','Blmngtn':'MedHigh','CollgCr':'MedHigh','ClearCr':'MedHigh',\n",
    "                                'Crawfor':'MedHigh','Somerst':'High','Timber':'High','Veenker':'High','GrnHill':'High',\n",
    "                                'NoRidge':'High','NridgHt':'High','StoneBr':'High'}, inplace = True)\n",
    "    \n",
    "    # one-hot encoding for other categorical variables\n",
    "    df = pd.get_dummies(df, columns = columns, drop_first = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns to create dummies from\n",
    "cols = ['ms_zoning','full_bath','sale_type','central_air','Location','garage_cars','exter_qual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "X = create_ames_dummies(X, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Note - the cells below show a few different combinations of variables I tested for my model. For each model, I chose one from 'numeric columns' and one from 'dummy columns' and proceeded with modeling (trying with and without polynomial features, and simple regression as well as some regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = ['overall_qual', 'garage_area','gr_liv_area','year_built','totrms_abvgrd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns2 = ['overall_qual','gr_liv_area','year_built','totrms_abvgrd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns3 = ['overall_qual','gr_liv_area','totrms_abvgrd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = ['StyleDummy','PosFeature',\n",
    "                 'full_bath_1','full_bath_2','full_bath_3','full_bath_4',\n",
    "                 'central_air_Y','Location_Low','Location_LowMed','Location_MedHigh',\n",
    "                 'garage_cars_1.0','garage_cars_2.0','garage_cars_3.0','garage_cars_4.0','garage_cars_5.0',\n",
    "                 'exter_qual_Fa','exter_qual_Gd','exter_qual_TA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns2 = ['StyleDummy','PosFeature',\n",
    "                 'central_air_Y','Location_Low','Location_LowMed','Location_MedHigh',\n",
    "                 'exter_qual_Fa','exter_qual_Gd','exter_qual_TA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns3 = ['central_air_Y','Location_Low','Location_LowMed','Location_MedHigh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns4 = ['Location_Low','Location_LowMed','Location_MedHigh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns5 = ['central_air_Y','Location_Low','Location_LowMed','Location_MedHigh',\n",
    "                 'exter_qual_Fa','exter_qual_Gd','exter_qual_TA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns6 = ['PosFeature','central_air_Y','Location_Low','Location_LowMed','Location_MedHigh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTION: Decision Point #1\n",
    "\n",
    "When defining 'selected_columns' below (the features to be used in all modeling below). Once this step has been"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = numeric_columns3 + dummy_columns5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: \n",
    "\n",
    "Here are some notes I took on various models I tested during the process. Leaving them here to show my process/how I evaluated and iterated.\n",
    "\n",
    "\n",
    "## Benchmark 1\n",
    "\n",
    "Numeric cols = overall qual, gr liv area, year built, totrms abvgrd\n",
    "\n",
    "Dummy cols = central air Y, location low, location lowmed, location medhigh\n",
    "\n",
    "Polynomials = yes, interaction only\n",
    "\n",
    "rMSE = 31265\n",
    "\n",
    "## Benchmark 2\n",
    "\n",
    "Numeric cols = overall qual, gr liv area, totrms abvgrd\n",
    "\n",
    "Dummy cols = central air Y, location low, location lowmed, location medhigh\n",
    "\n",
    "rMSE = 30254\n",
    "\n",
    "## Benchmark 3\n",
    "\n",
    "Numeric cols = overall qual, gr liv area, totrms abvgrd\n",
    "\n",
    "Dummy cols = central air Y, location low, location lowmed, location medhigh, exter qual vars\n",
    "\n",
    "rMSE = 30456 TOO MUCH VARIANCE - this model produced very different RMSE from one run to the next.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes:\n",
    "\n",
    "One thing I noticed was that, after thorough EDA and selecting the features which, from basic visualizations, suggested the strongest indication of house prices, I was able to make very simple models with low variance which could reliably produce an estimate with a root mean squared error ~32,000 and an R-squared of ~0.84. Adding even one or two dimensions on top of that pushed past a tipping point to an overfit model. When using polynomial features, including squared terms almost always produced an overfit, high-variance model, while using interaction only allowed me to include more variables and slightly reduce bias without sacrificing too much to variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select your matrix of predictors\n",
    "predictors = X[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTION: Decision Point #2\n",
    "\n",
    "Run the cells below to include polynomial features, or skip them to keep the basic model. For my final models, which produced the 'best' scores, I used polynomial features of degree 2 with interaction terms only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(include_bias = False, interaction_only = True)\n",
    "X_poly = poly.fit_transform(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the shape of your predictors matrix\n",
    "# Rule of thumb - if you have <30 observations per predictor, your model will likely be overfit\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTION: Decision Point #3\n",
    "\n",
    "## a) Linear Model\n",
    "\n",
    "To run a standard linear regression model, run the cells below. Or skip down to the regularization sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a basic linear regression model with these predictors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a simple model\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on training data\n",
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score on test data\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model on test set\n",
    "y_hat = lr.predict(X_test)\n",
    "\n",
    "# calculate a root mean squared error for your models predictions on the test set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_hat) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use statsmodels to see model output and further filter out variables based on their predictive power\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(X_train)\n",
    "\n",
    "# fit model to data and print out summary statistics\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTION: Decision Point #3\n",
    "\n",
    "## b) Regularization\n",
    "\n",
    "1. Import the modules\n",
    "2. Standarsize your predictor matrices (train and test)\n",
    "3. Run appropriate CV to find optimal hyperparameters (alpha, l1 ratio)\n",
    "4. Fit the desired model - Ridge, Lasso, or ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "kf = KFold(n_splits = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = ss.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = ss.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic Net\n",
    "\n",
    "I was not sure which direction to take with regularization. After starting with lasso (and not finding it effective - see below), I settled on elastic net for one of my two submission models. I applied elastic net to one of my 'overfit' models (numeric columns3 + dummy_columns5) as I did not think that the high bias models would benefit as much from regularization.\n",
    "\n",
    "Run the cells below to use elastic net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CV to find 'optimal' alpha and l1 ratio, then fit the model with those hyperparameters\n",
    "elastic_net_model = ElasticNetCV(l1_ratio = [.1, .5, .7, .9, .95, .99, 1], cv = kf, max_iter= 5000)\n",
    "elastic_net_model = elastic_net_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save chosen hyperparameters\n",
    "alpha = elastic_net_model.alpha_\n",
    "l1_ratio = elastic_net_model.l1_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with chosen hyperparameters\n",
    "elastic_net = ElasticNet(alpha = alpha, l1_ratio = l1_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(elastic_net, X_train_scaled, y_train, cv = kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Looking at the CV scores, the elastic net does not appear to solve the 'high variance' problem and appears to perform worse than my simpler model with no regularization (which had fewer dummy variables). Also I realize here that I am potentially overdoing the 'train test split' by running CV on a subset of my training data. I imagine I might get slightly better results using more of my training data, but that my problem lies in model design. Also, when fitting my elastic net model, I needed to increase the number of iterations for the model to settle on optimal hyperparameters. This is further evidence that I need to do more work on my feature selection and model design before resorting to regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Model\n",
    "\n",
    "Run the cells below to apply lasso regularization. I did not find much success with lasso. The resulting model suffered from high variance and the fact that the alpha chosen by LassoCV was so high leads me to believe that there are problems with my model design that go beyond simply finding and throwing out redundant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_model = LassoCV(cv = kf)\n",
    "lasso_model = lasso_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_optimal_alpha = lasso_model.alpha_\n",
    "lasso_optimal_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_quick = Lasso(alpha = lasso_optimal_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(lasso_quick, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge\n",
    "\n",
    "Run the cells below to apply ridge regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = RidgeCV(cv = kf)\n",
    "ridge_model = ridge_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_optimal_alpha = ridge_model.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_quick = Ridge(ridge_optimal.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(ridge_quick, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a basic linear regression model with these predictors\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a simple model\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "# fit the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# score on training data\n",
    "lr.score(X_train, y_train)\n",
    "\n",
    "# score on test data\n",
    "lr.score(X_test, y_test)\n",
    "\n",
    "y_hat = lr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test, y_hat) ** 0.5\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X = sm.add_constant(X_train)\n",
    "\n",
    "# fit model to data and print out summary statistics\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "Run the cells below to load in the test data, clean and prepare it using the same procedure as on the training data, and use the model that you fit above to predict salesprice for the testing data. Finally, save your predictions to a csv file in the correct format for uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data, clean it, then predict salesprice using model developed above\n",
    "ames_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am applying a function I've written to inspect a data set (see Data Handling Notebook) to inspect the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to handle basic data inspection\n",
    "def inspect_data(df):\n",
    "    '''Perfrom basic data inspection tasks, provide user with readable output'''\n",
    "    print('Shape: {}\\n'.format(df.shape))\n",
    "    print(df.info(),'\\n') \n",
    "    print('Null Counts: \\n',df.isnull().sum(),'\\n')\n",
    "    \n",
    "    return df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_data(ames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "clean_ames_data(ames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same dummy variables for your test data\n",
    "ames_test = create_ames_dummies(ames_test, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select your predictors matrix\n",
    "test_predictors = ames_test[selected_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Handling Missing Columns in Test Data\n",
    "\n",
    "The below cells (commented out) show how I would handle missing columns in my test data (certain categories missing). I would have received an error when trying to select feature columns if the column was missing - so how I handled this (in cases where it came up) is to create a list of missing columns and run a for loop to create and populate the missing columns in the test data. In the final models that I developed and used for the competition, I did not need to use this functionality as I did not run into the missing columns problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing_cols = ['ms_zoning_C (all)','totrms_abvgrd_3','totrms_abvgrd_13','totrms_abvgrd_14',\n",
    " #          'totrms_abvgrd_15','electrical_Mix','heating_OthW','heating_Wall','garage_cars_5.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in missing_cols:\n",
    " #   ames_test[col] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run these cells if you are using polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poly = poly.fit_transform(test_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poly.shape # check that the shape (number of columns) matches your training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to standardize if you are using the lasso, ridge, or elastic net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_poly_scaled = ss.fit_transform(test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ames_test['salesprice_hat'] = elastic_net_model.predict(test_poly_scaled) # predict your target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-format Id and SalePrice columns to meet Kaggle requirements\n",
    "ames_test.rename({'id': 'Id', 'salesprice_hat': 'SalePrice'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export predictions to csv\n",
    "predictions = ames_test[['Id','SalePrice']].to_csv('../data/submission_22.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "My final submissions were the output from two models. The first was a relatively simple model that I came to after some trial and error, and realizing that trying to add too much complexity very quickly leads to overfitting. This model included the following explanatory variables:\n",
    "\n",
    "1. Quantitative variables for overall condition, total rooms above grade, and above ground living area.\n",
    "2. Dummy variables for central air, and for which locations subgroup the house was in (I divided the neighborhoods into four groups based on median house price: Low - below the 25th percentile, LowMed - between the 25th percentile and median, MedHigh - between the median and 75th percentile, High (the excluded dummy) - above the 75th percentile .\n",
    "3. Pairwise interaction terms between all of the above.\n",
    "\n",
    "This model reliably produced r-squared scores of approximately 0.84 and RMSE around 30,000. I believe the model suffers from high bias due to its simplicity, and I found that adding even a few more explanatory variables greatly increased the variance. Including squared terms generally increased variance beyond an acceptable point (negative r-squared on testing data for example).\n",
    "\n",
    "In order to correct for some of the variance introduced by a slightly more complex model, my second selected submission was from a model that added dummy variables for external quality while applying elastic net regularization. The model did not perform better than my simpler model.\n",
    "\n",
    "## Main Takeaways\n",
    "\n",
    "1. I had more success with simpler models that could reliably produce estimates, albeit with higher bias than I would like.\n",
    "2. Adding slightly more variables to the model without filtering out which interaction terms are useful rapidly leads to an overly complex model.\n",
    "3. Creating dummy variables to account for location in a simpler way than the neighborhood categories greatly improved my model. I came to this idea after visualizing the relationships I expected (i.e. a larger house should be more expensive) and investigating the outliers to understand why the relationship didn't hold in all cases. More of this kind of deep-dive analysis is likely the key to a better model, and something I will spend more time on in the future.\n",
    "4. A couple of dummy variables I expected to be extremely valuable, one for a particular house style, and one indicating that the house is near or adjacent to a 'positive feature' (such as a park), were much less predictive (high p-values) than I expected, and I ended up not using them in my final models. I am curious why a relationship which stood out so clearly during my EDA was not helpful in my modeling, and would like to pursue this further.\n",
    "5. Using the tools at my disposal, I reached a limit to how complex I could make my model without severly overfitting - there was an upper bound on the r-squared I was able to achieve consistently. This tells me I have room for improvement in the following areas. \n",
    "    * Better model design - understanding which variables to choose, which to throw out, and which interaction terms and/or polynomial features will help. \n",
    "    * Applying regularization - when my efforts resulted in overfitting, I tried to use regularization techniques, with little success. I do not feel confident that I understand when and how to apply these tools. In the context of the analysis above, I think I fell into the trap of 'throwing models at the problem' and need to improve my understanding of these tools to be able to use them responsibly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
